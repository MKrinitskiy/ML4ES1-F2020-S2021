# Список тем к зачету за II семестр.

### Секция 1 - "классические" методы МО

- Задача бинарной классификации: вероятностная формулировка; байесовский классификатор (БК); способ обучения БК; решающее правило БК; свойства БК;
- Задача бинарной классификации: вероятностная формулировка, наивный байесовский классификатор (НБК); способ обучения НБК; решающее правило НБК;
- Задача бинарной классификации: вероятностная формулировка, модели линейного дискриминантного анализа (LDA) и квадратичного дискриминантного анализа (QDA): формулировка, способ обучения, решающее правило, разделяющая поверхность, основные свойства;
- Задача классификации: вероятностная формулировка; понятие разделяющей поверхности, уравнение разделяющей поверхности для произвольной вероятностной модели;
- Задача бинарной классификации: вероятностная формулировка; модель логистической регрессии: формулировка, функция потерь, способ обучения, разделяющая поверхность, решающее правило, основные свойства;
- Задача мультиномиальной классификации: вероятностная формулировка; модель мультиномиальной логистической регрессии: формулировка, функция потерь, способ обучения, разделяющая поверхность, решающее правило, основные свойства;
- Оценка качества моделей классификации: меры качества, базовые сценарии их применения;
- Обобщенные линейные модели (GLM): формулировка; линейная регрессия и логистическая регрессия (бинарная и мультиномиальная) как GLM; связь вида функции потерь с видом функции  связи GLM;
- Обобщенные аддитивные модели (GAM): формулировка; линейная регрессия и логистическая регрессия (бинарная и мультиномиальная) как GAM; связь вида функции потерь с видом функции  связи GAM и с видом преобразующих функций GAM;
- Метод опорных векторов (SVM) в задаче классификации: формулировка, схема работы, способ обучения; kernel trick; основные свойства SVM;
- Непараметрические методы МО. Деревья решений: формулировка, схема работы, функции ошибки, способ обучения, основные свойства;



### Секция 2 - искусственные нейронные сети и градиентные методы оптимизации

- Многослойный перцептрон (MLP): формулировка, схема работы, функциональная запись, графическое представление; MLP как обобщенная линейная модель, MLP как обобщенная аддитивная модель; варианты функции потерь MLP в задачах регрессии и классификации;
- Многослойный перцептрон (MLP): формулировка, схема работы, функциональная запись, графическое представление; подсчет количества параметров MLP; способ обучения MLP
- Оптимизация функции ошибки в задаче линейной регрессии. Варианты оптимизации (стохастический и "обычный"). Предпосылки для градиентной оптимизации. Основной принцип градиентной оптимизации функции ошибки в задачах машинного обучения.
- Градиентная оптимизация функции ошибки. Основной принцип и особенности в сравнении с аналитическим решением на примере задачи линейной регрессии. Гиперпараметры градиентной оптимизации. 
- Градиентная оптимизация функции ошибки. Основной принцип. Гиперпараметры градиентной оптимизации. Особенности градиентной оптимизации в условиях мультиколлинеарности признаков линейной модели.
- Градиентная оптимизация функции ошибки. Основной принцип. Ландшафт функции ошибки. Решение проблемы больших данных. Особенности стохастической градиентной оптимизации в сравнении с градиентной оптимизацией.
- Стохастическая градиентная оптимизация функции ошибки. Основной принцип. Ландшафт функции ошибки. Особенности стохастической градиентной оптимизации. Проблемы сходимости и подходы улучшения сходимости. Гиперпараметры стохастической градиентной оптимизации.
